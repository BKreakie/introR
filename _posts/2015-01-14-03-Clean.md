---
title: "03 - Clean"
author: Jeffrey W. Hollister
layout: post_page
---
In this third lesson we are going to start working on maninpulating and cleaning up our data frames.  We are spending some significant time on this because, in my experience, most data analysis and statistics classes seem to assume that 95% of the time spent working with data is on the analysis and interpretation of that analysis and little time is spent getting data ready to analyze.  However, in reality, I'd argue, the time spent is flipped with most time spent on cleaning up data and significantly less time on the analysis.  Before we jump into the lesson, quick links and lesson goals are:

##Quick Links to Exercises and R code
- [Lesson 3 R Code](/gedr/rmd_posts/2015-01-14-03-Clean.R): All the code from this post in an R Script.
- [Exercise 1](#exercise-1): Subsetting the NLA data
- [Exercise 2](#exercise-2): Merge two NLA data files together
- [Exercise 3](#exercise-3): Reshaping and modifying to get NLA summary stats

##Lesson Goals
- Understand and use various subsetting methods (indexing and `subset()`).
- Be able to merge data frames by simply combining rows and columns
- Use `merge()` to combine data frames by a common key.
- Do some basic reshaping and modifying data frames.

##Subset
Cleaning data (or data munging, data jujitsu, data wrangling) often happens at two points.  The first stage is getting raw data files ready to be read in by the computer.  This can be quite the challenge.  For the purposes of this workshop we are not going to focus on that side too much.  The data files we are using are in a clean enough format to be read in directly.  If you want to learn more about some best practices in maintaining good clean data files, I'd point you to two places.  First is some materials from [Data Carpentry on working with spreadsheets](https://github.com/datacarpentry/datacarpentry/blob/master/lessons/excel/ecology_spreadsheets.md) and second is [Hadley Wickham's Tidy Data paper](http://www.jstatsoft.org/v59/i10/paper).

The second point where you need to clean data is to get it ready for specific analysis and that is what this lesson will focus on.  Lets start exploring this with using indexing to subset both observations (the rows) and variables (the columns).  Some of the commands we know already, like `head()` and `tail()` kind of do this, but we need more control than that.  We get that with indexing.  In short indexing allows you to specify individual (or ranges) of rows and columns.  We can index data frames and vectors (and lists and matrices, but we aren't going to go into that).  Let's start with a vector example and build from there.


{% highlight r %}
#Create a vector
x<-c(10:19)
x
{% endhighlight %}



{% highlight text %}
##  [1] 10 11 12 13 14 15 16 17 18 19
{% endhighlight %}



{% highlight r %}
#Postive indexing returns just the value in the ith place
x[7]
{% endhighlight %}



{% highlight text %}
## [1] 16
{% endhighlight %}



{% highlight r %}
#Negative indexing returns all values except the value in the ith place
x[-3]
{% endhighlight %}



{% highlight text %}
## [1] 10 11 13 14 15 16 17 18 19
{% endhighlight %}



{% highlight r %}
#Ranges work too
x[8:10]
{% endhighlight %}



{% highlight text %}
## [1] 17 18 19
{% endhighlight %}



{% highlight r %}
#A vector can be used to index
#Can be numeric
x[c(2,6,10)]
{% endhighlight %}



{% highlight text %}
## [1] 11 15 19
{% endhighlight %}



{% highlight r %}
#Can be boolean - will repeat the pattern 
x[c(TRUE,FALSE)]
{% endhighlight %}



{% highlight text %}
## [1] 10 12 14 16 18
{% endhighlight %}



{% highlight r %}
#Can even get fancy
x[x%%2==0]
{% endhighlight %}



{% highlight text %}
## [1] 10 12 14 16 18
{% endhighlight %}
 
So that's kinda handy.  In practice you wouldn't likely just be asking for that information to be returned to the screen, you'd probably save the output to a new object, kind of like `evens<-x[x%%2==0]`.  Let's move on now and look at indexing data frames.

Since a data frame has two dimensions, you need to specify an index for both the row and the column.  You can specify both and get a single value like `data_frame[row,column`],specify just the row and the get the whole row back like `data_frame[row,]` or get just the column with `data_frame[,column]`.  Let's work some examples.


{% highlight r %}
#Let's use one of the stock data frames in R, iris
head(iris)
{% endhighlight %}



{% highlight text %}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1          5.1         3.5          1.4         0.2  setosa
## 2          4.9         3.0          1.4         0.2  setosa
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
{% endhighlight %}



{% highlight r %}
#And grab a specific value
iris[1,1]
{% endhighlight %}



{% highlight text %}
## [1] 5.1
{% endhighlight %}



{% highlight r %}
#A whole column
petal_len<-iris[,3]
petal_len
{% endhighlight %}



{% highlight text %}
##   [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3
##  [18] 1.4 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4
##  [35] 1.5 1.2 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7
##  [52] 4.5 4.9 4.0 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1
##  [69] 4.5 3.9 4.8 4.0 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5
##  [86] 4.5 4.7 4.4 4.1 4.0 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1
## [103] 5.9 5.6 5.8 6.6 4.5 6.3 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9
## [120] 5.0 5.7 4.9 6.7 4.9 5.7 6.0 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1
## [137] 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9 5.7 5.2 5.0 5.2 5.4 5.1
{% endhighlight %}



{% highlight r %}
#A row
obs15<-iris[15,]
obs15
{% endhighlight %}



{% highlight text %}
##    Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 15          5.8           4          1.2         0.2  setosa
{% endhighlight %}



{% highlight r %}
#Many rows
obs3to7<-iris[3:7,]
obs3to7
{% endhighlight %}



{% highlight text %}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 3          4.7         3.2          1.3         0.2  setosa
## 4          4.6         3.1          1.5         0.2  setosa
## 5          5.0         3.6          1.4         0.2  setosa
## 6          5.4         3.9          1.7         0.4  setosa
## 7          4.6         3.4          1.4         0.3  setosa
{% endhighlight %}

So that's the basics.  Still in the arena of kinda useful, but not really... Remember though that we can index vectors with other vectors and a data frame is just a collection of vectors (kinda) and that starts to get interesting.  Also remember that data frames have column names.  We can use those too.  Let's try it.


{% highlight r %}
#First, there are a couple of ways to use the column names
iris$Petal.Length
{% endhighlight %}



{% highlight text %}
##   [1] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 1.5 1.6 1.4 1.1 1.2 1.5 1.3
##  [18] 1.4 1.7 1.5 1.7 1.5 1.0 1.7 1.9 1.6 1.6 1.5 1.4 1.6 1.6 1.5 1.5 1.4
##  [35] 1.5 1.2 1.3 1.4 1.3 1.5 1.3 1.3 1.3 1.6 1.9 1.4 1.6 1.4 1.5 1.4 4.7
##  [52] 4.5 4.9 4.0 4.6 4.5 4.7 3.3 4.6 3.9 3.5 4.2 4.0 4.7 3.6 4.4 4.5 4.1
##  [69] 4.5 3.9 4.8 4.0 4.9 4.7 4.3 4.4 4.8 5.0 4.5 3.5 3.8 3.7 3.9 5.1 4.5
##  [86] 4.5 4.7 4.4 4.1 4.0 4.4 4.6 4.0 3.3 4.2 4.2 4.2 4.3 3.0 4.1 6.0 5.1
## [103] 5.9 5.6 5.8 6.6 4.5 6.3 5.8 6.1 5.1 5.3 5.5 5.0 5.1 5.3 5.5 6.7 6.9
## [120] 5.0 5.7 4.9 6.7 4.9 5.7 6.0 4.8 4.9 5.6 5.8 6.1 6.4 5.6 5.1 5.6 6.1
## [137] 5.6 5.5 4.8 5.4 5.6 5.1 5.1 5.9 5.7 5.2 5.0 5.2 5.4 5.1
{% endhighlight %}



{% highlight r %}
head(iris["Petal.Length"])
{% endhighlight %}



{% highlight text %}
##   Petal.Length
## 1          1.4
## 2          1.4
## 3          1.3
## 4          1.5
## 5          1.4
## 6          1.7
{% endhighlight %}



{% highlight r %}
#Multiple colums
head(iris[c("Petal.Length","Species")])
{% endhighlight %}



{% highlight text %}
##   Petal.Length Species
## 1          1.4  setosa
## 2          1.4  setosa
## 3          1.3  setosa
## 4          1.5  setosa
## 5          1.4  setosa
## 6          1.7  setosa
{% endhighlight %}



{% highlight r %}
#Now we can combine what we have seen to do some more complex queries
#Lets get all the data for Species with a petal length greater than 6
big_iris<-iris[iris$Petal.Length>=6,]
head(big_iris)
{% endhighlight %}



{% highlight text %}
##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species
## 101          6.3         3.3          6.0         2.5 virginica
## 106          7.6         3.0          6.6         2.1 virginica
## 108          7.3         2.9          6.3         1.8 virginica
## 110          7.2         3.6          6.1         2.5 virginica
## 118          7.7         3.8          6.7         2.2 virginica
## 119          7.7         2.6          6.9         2.3 virginica
{% endhighlight %}



{% highlight r %}
#Or maybe we want just the sepal widths of the virginica species
virginica_iris<-iris$Sepal.Width[iris$Species=="virginica"]
head(virginica_iris)
{% endhighlight %}



{% highlight text %}
## [1] 3.3 2.7 3.0 2.9 3.0 3.0
{% endhighlight %}

Got it?!  Probably sort of!?  I will admit that the syntax for subsetting data this way is a bit obtuse.  I did want to make sure you all saw it though becuase it comes up often in other peoples code and helps you get a better understanding, I think, of what you are doing.  That being said there are other options.  In base, `subset()` and also in `dplyr` which will be the next lesson.


{% highlight r %}
#And redo what we did above
big_iris_subset<-subset(iris,subset=Petal.Length>=6)
head(big_iris_subset)
{% endhighlight %}



{% highlight text %}
##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species
## 101          6.3         3.3          6.0         2.5 virginica
## 106          7.6         3.0          6.6         2.1 virginica
## 108          7.3         2.9          6.3         1.8 virginica
## 110          7.2         3.6          6.1         2.5 virginica
## 118          7.7         3.8          6.7         2.2 virginica
## 119          7.7         2.6          6.9         2.3 virginica
{% endhighlight %}



{% highlight r %}
virginica_iris_subset<-subset(iris,subset=Species=="virginica",select=Sepal.Width)
head(virginica_iris_subset)
{% endhighlight %}



{% highlight text %}
##     Sepal.Width
## 101         3.3
## 102         2.7
## 103         3.0
## 104         2.9
## 105         3.0
## 106         3.0
{% endhighlight %}

So, I think that is a big improvement in the syntax over the raw indexing and certainly lends itself to more understandable code.

##Exercise 1
This exercise is going to focus on using what we just covered on indexing and subsetting to start to clean up the National Lakes Assessment data files.  Remember to use the stickies: green when your done, red if you have a problem.

1. If it isn't already open, make sure you have the script we created, "nla_analysis.R" opened up.
2. Start a new section of code in this script by simply putting in a line or two of comments indicating what it is this set of code does.
3. Our goal for this is to create two new data frames that represent a subset of the observations as well as a subset of the data. 
4. First, from the  `nla_sites` data frame we want a new data frame that has only the following columns: SITE_ID, LON_DD, LAT_DD, STATE_NAME, WSA_ECO9, NUT_REG, NUTREG_NAME, LAKE_ORIGIN, and RT_NLA.  Name the new data frame `nla_sites_subset`.
5. Next, lets subset the water quality data from `nla_wq`.  The columns we want for this are: SITE_ID, VISIT_NO, SITE_TYPE, TURB, NTL, PTL, CHLA, and SECMEAN. Call this `nla_wq_subset`.
6. Last thing we are going to need to do is get a subset of the observations from `nla_wq_subset`.  We need only the lakes that with VISIT_NO equal to 1 and SITE_TYPE equal to "REF_Lake".  Keep the same name, `nla_wq_subset`, for this data frame.

##Merging Data
Another very common clean-up/data managment task is to be able to merge data into a single data frame.  We are going to talk about several different ways to do this.  First, is adding a new column or existing vector to a data frame with `cbind()`,`data.frame()`,or`$`. 


{% highlight r %}
#Create a new vector to add to the iris data frame
categories<-sample(1:3,nrow(iris),replace=T)
iris_cbind<-cbind(iris,categories)
head(iris_cbind)
{% endhighlight %}



{% highlight text %}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species categories
## 1          5.1         3.5          1.4         0.2  setosa          3
## 2          4.9         3.0          1.4         0.2  setosa          1
## 3          4.7         3.2          1.3         0.2  setosa          1
## 4          4.6         3.1          1.5         0.2  setosa          1
## 5          5.0         3.6          1.4         0.2  setosa          3
## 6          5.4         3.9          1.7         0.4  setosa          3
{% endhighlight %}



{% highlight r %}
#Can also use data.frame again for this
iris_cbind_df<-data.frame
head(iris_cbind_df)
{% endhighlight %}



{% highlight text %}
##                                                                            
## 1 function (..., row.names = NULL, check.rows = FALSE, check.names = TRUE, 
## 2     stringsAsFactors = default.stringsAsFactors())                       
## 3 {                                                                        
## 4     data.row.names <- if (check.rows && is.null(row.names))              
## 5         function(current, new, i) {                                      
## 6             if (is.character(current))
{% endhighlight %}



{% highlight r %}
#Direct assignment
iris_cbind_dollar<-iris
iris_cbind_dollar$categories<-sample(1:3,nrow(iris),replace=T)
head(iris_cbind_dollar)
{% endhighlight %}



{% highlight text %}
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species categories
## 1          5.1         3.5          1.4         0.2  setosa          2
## 2          4.9         3.0          1.4         0.2  setosa          3
## 3          4.7         3.2          1.3         0.2  setosa          2
## 4          4.6         3.1          1.5         0.2  setosa          3
## 5          5.0         3.6          1.4         0.2  setosa          1
## 6          5.4         3.9          1.7         0.4  setosa          3
{% endhighlight %}

These are all handy to know, but you need to be very careful with it.  First, the data in the vector you are adding needs to be in the same order as the observations in the data frame.  Also it needs to have the same number of rows (or divisible by that number of rows).  

The next thing we might want to do is add some new rows to a data frame.  This is very handy as you might have data collected and entered at one time, and then additional observations made later that need to be added.  So with `rbind()` we can stack two data frames with the same columns to store more observations.  


{% highlight r %}
#Let's first create a new small example data.frame
rbind_df<-data.frame(a=1:3,b=c("a","b","c"),c=c(T,T,F),d=rnorm(3))
#Now an example df to add
rbind_df2<-data.frame(a=10:12,b=c("x","y","z"),c=c(F,F,F),d=rnorm(3))
rbind_df<-rbind(rbind_df, rbind_df2)
rbind_df
{% endhighlight %}



{% highlight text %}
##    a b     c          d
## 1  1 a  TRUE -1.1354252
## 2  2 b  TRUE -0.4540050
## 3  3 c FALSE -1.6174040
## 4 10 x FALSE -0.3993137
## 5 11 y FALSE  1.8285269
## 6 12 z FALSE -0.2683445
{% endhighlight %}

Now something to think about.  Could you add a vector as a new row?  Why/Why not? When/When not?

Let's go back to the columns now.  What we can currently do is add columns that need to be same length, or length that is divisible by number of rows in the original, and need to be in the same order.  But it is very common to have to datasets that are in different orders and have differing numbers of rows.  What we want to do in that case is going to be more of a database type function and join two tables based on a common column.  A common way to do that in base R is with `merge()`.   So let's contrive another example by creating a dataset to merge to `rbind_df` that we created above.


{% highlight r %}
# Contrived data frame
rbind_df_merge_me<-data.frame(a=c(1,3,10,11,14,6,23),x=rnorm(7),names=c("bob","joe","sue",NA,NA,"jeff",NA))
# Create merge of matches
rbind_df_merge_match<-merge(rbind_df,rbind_df_merge_me,by="a")
rbind_df_merge_match
{% endhighlight %}



{% highlight text %}
##    a b     c          d         x names
## 1  1 a  TRUE -1.1354252 -1.337627   bob
## 2  3 c FALSE -1.6174040 -1.672427   joe
## 3 10 x FALSE -0.3993137 -1.063296   sue
## 4 11 y FALSE  1.8285269  2.055271  <NA>
{% endhighlight %}



{% highlight r %}
# Create merge of matches and all of the first data frame
rbind_df_merge_allx<-merge(rbind_df,rbind_df_merge_me,by="a",all.x=TRUE)
rbind_df_merge_allx
{% endhighlight %}



{% highlight text %}
##    a b     c          d         x names
## 1  1 a  TRUE -1.1354252 -1.337627   bob
## 2  2 b  TRUE -0.4540050        NA  <NA>
## 3  3 c FALSE -1.6174040 -1.672427   joe
## 4 10 x FALSE -0.3993137 -1.063296   sue
## 5 11 y FALSE  1.8285269  2.055271  <NA>
## 6 12 z FALSE -0.2683445        NA  <NA>
{% endhighlight %}

##Exercise 2
In this exercise we are going to practice merging our NLA data.  You will remember that we have two datasets, one on water quality and one on site information.  We selected some info out of the water quality data that we didn't select out of the site info so we have two data frames, with differing numbers of rows and unknown order.  Use your stickies!

1. This is the only task we have for this exercise.  Add to your script a line (or more if you need it) to create a new data frame, `nla_data`, that is a merge of `nla_site_subset` and `nla_wq_subset`, but with all lines in `nla_wq_subset` preserved in the output.
2. If that goes quickly, feel free to explore `rbind()` some.

##Reshape and Modify
So we now have a data frame that is set up pretty well to facilitate some future analysis.  That being said, different analyses and visualizations are probably going to require some additional monekying around with our data.  This requires us to reshape, modify, aggregate, or split up our dataset.  There are many ways to do this and a lot depends on the format of the original data.  We will focus on a couple: transposing and aggregation.  There is a lot more you can do.  Again, the best source for packages and examples on this is Hadley Wickham.  In particular I would look at the `reshape` package and the associated links:

- [`rehsape` pakcage](http://had.co.nz/reshape/)
- [JSS paper](http://www.jstatsoft.org/v21/i12/paper)

We will look at a couple of Hadley's packages, `dplyr` and `reshape`, soon, but right now let's look at some basic reshaping in base R.  We will show how to transpose and aggregate.  We already know how to do basic subsetting.  These techniques will add to our toolbox.

To transpose a data frame all we need is the `t()` function. 


{% highlight r %}
#Look at are contrived example again
rbind_df_merge_match
{% endhighlight %}



{% highlight text %}
##    a b     c          d         x names
## 1  1 a  TRUE -1.1354252 -1.337627   bob
## 2  3 c FALSE -1.6174040 -1.672427   joe
## 3 10 x FALSE -0.3993137 -1.063296   sue
## 4 11 y FALSE  1.8285269  2.055271  <NA>
{% endhighlight %}



{% highlight r %}
#And the transpose of this
t(rbind_df_merge_match)
{% endhighlight %}



{% highlight text %}
##       [,1]         [,2]         [,3]         [,4]        
## a     " 1"         " 3"         "10"         "11"        
## b     "a"          "c"          "x"          "y"         
## c     " TRUE"      "FALSE"      "FALSE"      "FALSE"     
## d     "-1.1354252" "-1.6174040" "-0.3993137" " 1.8285269"
## x     "-1.337627"  "-1.672427"  "-1.063296"  " 2.055271" 
## names "bob"        "joe"        "sue"        NA
{% endhighlight %}

I find that I don't use transpose a lot, but when I do it is usually in combination with other summary and aggregation methods.  It can also be useful with data in a long form where each row does not represent a single observation and the observation id is duplicated.  Getting that to work with base R takes some work. The `reshape()` function can be used for that, but to be honest, I have never used it.  Our next lesson should provide some better tools for doing this.

Let's now look at some aggregation commands.  We will focus on the wrapper functions `aggregate()` and `by()`.  These provide a more convenient way to access the `apply` family of functions.  I will introduce those, but not go into too much detail.  There are many apply functions and they include: `apply()`, `lapply()`, `sapply()`, `mapply()`, `tapply()`,  and `eapply()` (I think that is all of them...).  There are also two convenience function `aggregate()`, and `by()` that are good to know.  

I have never found the apply functions or their helper to be very intuitive and each seems to have it's quirks with the data structures the like as input or return as output.  Often we simply want to input a data frame, summarize or split based on a column in that data frame and get a data frame returned.  This is what `dplyr` does and what we will focus on in the next lesson.  That being said, some of the base R function do work well.  We will look specifically at `apply()`, `tapply()`, and `aggregate()`

`apply()` allows you to run a function by row or by columns. It looks for a data frame as input and returns a vector.  For instance, you could have multiple columns that you wanted get an average across.  Perhaps each row is a site where we measured weekly temperature.  To get an average monthly temperature we could do:


{% highlight r %}
temp_df<-data.frame(id=1:100,week1=runif(100,20,25), week2=runif(100,19,24), 
                    week3=runif(100,18,26), week4=runif(100,17,23))
head(temp_df)
{% endhighlight %}



{% highlight text %}
##   id    week1    week2    week3    week4
## 1  1 21.10484 22.71817 25.04452 18.10035
## 2  2 24.82024 19.83497 22.24966 22.24890
## 3  3 23.52537 20.11728 24.31076 21.05278
## 4  4 22.70667 21.70813 22.23503 21.01845
## 5  5 24.63803 19.47076 19.33976 21.08882
## 6  6 21.62090 23.80053 23.06171 20.82204
{% endhighlight %}



{% highlight r %}
apply(temp_df,1,mean)
{% endhighlight %}



{% highlight text %}
##   [1] 17.59358 18.23075 18.40124 18.33366 17.90747 19.06104 19.06291
##   [8] 19.31947 18.77476 19.49413 20.94668 19.45764 19.05749 20.41977
##  [15] 19.84586 20.75937 22.02836 21.95663 20.79436 19.40653 20.75159
##  [22] 22.29255 21.65447 21.74247 21.88683 22.98349 21.58938 22.23665
##  [29] 24.18628 23.07552 23.28737 25.34386 23.75765 23.98183 24.53480
##  [36] 24.15878 25.20611 25.14014 23.73032 24.44252 24.79877 25.20485
##  [43] 25.99173 26.08672 26.02384 25.69929 27.52637 25.32987 26.66385
##  [50] 27.63503 27.49337 28.47505 27.92889 27.89963 29.01520 27.89304
##  [57] 28.53081 29.85160 29.81541 29.52090 30.79776 29.59143 30.41239
##  [64] 29.38147 30.45746 30.39474 29.71411 30.37956 30.47486 30.82763
##  [71] 30.20854 32.08658 31.69423 32.46435 32.01620 32.34741 32.65793
##  [78] 32.37226 33.20549 32.71165 31.78572 32.07120 35.03264 34.31066
##  [85] 34.71805 34.62055 35.82550 35.47986 34.90418 35.07369 35.56723
##  [92] 36.86193 36.33255 36.67320 34.72761 36.45493 35.99679 38.02898
##  [99] 35.55552 37.79275
{% endhighlight %}



{% highlight r %}
temp_df$month_mean_temp <- apply(temp_df,1,mean)
head(temp_df)
{% endhighlight %}



{% highlight text %}
##   id    week1    week2    week3    week4 month_mean_temp
## 1  1 21.10484 22.71817 25.04452 18.10035        17.59358
## 2  2 24.82024 19.83497 22.24966 22.24890        18.23075
## 3  3 23.52537 20.11728 24.31076 21.05278        18.40124
## 4  4 22.70667 21.70813 22.23503 21.01845        18.33366
## 5  5 24.63803 19.47076 19.33976 21.08882        17.90747
## 6  6 21.62090 23.80053 23.06171 20.82204        19.06104
{% endhighlight %}

This is useful, but lets think a bit about the constraints.  Since the only grouping that is done is by row or column, the function you apply to each row or to each column needs to make sense for the data types that are stored in the rows and columns.  For instance, if you ask for a mean across all the rows and one of your rows contains character values, that is going to cause problems. 

Our next command is `tapply()` which works with vectors.  You input a vector, supply factors of the same length as your input, and then a function to apply.  We can go back to the `iris` data and play with this.


{% highlight r %}
sepal_len<-iris$Sepal.Length
tapply(sepal_len,iris$Species,mean)
{% endhighlight %}



{% highlight text %}
##     setosa versicolor  virginica 
##      5.006      5.936      6.588
{% endhighlight %}



{% highlight r %}
#What if we need to use function arguments?
sepal_len[10]<-NA
mean(sepal_len)
{% endhighlight %}



{% highlight text %}
## [1] NA
{% endhighlight %}



{% highlight r %}
mean(sepal_len, na.rm=TRUE)
{% endhighlight %}



{% highlight text %}
## [1] 5.849664
{% endhighlight %}



{% highlight r %}
tapply(sepal_len,iris$Species,mean)
{% endhighlight %}



{% highlight text %}
##     setosa versicolor  virginica 
##         NA      5.936      6.588
{% endhighlight %}



{% highlight r %}
tapply(sepal_len,iris$Species,function(x) mean(x, na.rm=T))
{% endhighlight %}



{% highlight text %}
##     setosa versicolor  virginica 
##   5.008163   5.936000   6.588000
{% endhighlight %}

So that is useful, but what we really want is to do this these things across multiple groups and multiple columns.  The last base R command that we are going to discuss, `aggregate()`,  does this.  This function will accept a data frame as input, requires a list of your grouping variables, and then the function to apply.  Back to `iris`...


{% highlight r %}
aggregate(iris,iris$Species,mean)
{% endhighlight %}



{% highlight text %}
## Error in aggregate.data.frame(iris, iris$Species, mean): 'by' must be a list
{% endhighlight %}



{% highlight r %}
aggregate(iris,list(iris$Species),mean)
{% endhighlight %}



{% highlight text %}
## Warning in mean.default(X[[1L]], ...): argument is not numeric or logical:
## returning NA
{% endhighlight %}



{% highlight text %}
## Warning in mean.default(X[[2L]], ...): argument is not numeric or logical:
## returning NA
{% endhighlight %}



{% highlight text %}
## Warning in mean.default(X[[3L]], ...): argument is not numeric or logical:
## returning NA
{% endhighlight %}



{% highlight text %}
##      Group.1 Sepal.Length Sepal.Width Petal.Length Petal.Width Species
## 1     setosa        5.006       3.428        1.462       0.246      NA
## 2 versicolor        5.936       2.770        4.260       1.326      NA
## 3  virginica        6.588       2.974        5.552       2.026      NA
{% endhighlight %}



{% highlight r %}
aggregate(iris[,-5],list(iris$Species),mean)
{% endhighlight %}



{% highlight text %}
##      Group.1 Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1     setosa        5.006       3.428        1.462       0.246
## 2 versicolor        5.936       2.770        4.260       1.326
## 3  virginica        6.588       2.974        5.552       2.026
{% endhighlight %}



{% highlight r %}
species_means<-aggregate(iris[,-5],list(iris$Species),mean)
species_means
{% endhighlight %}



{% highlight text %}
##      Group.1 Sepal.Length Sepal.Width Petal.Length Petal.Width
## 1     setosa        5.006       3.428        1.462       0.246
## 2 versicolor        5.936       2.770        4.260       1.326
## 3  virginica        6.588       2.974        5.552       2.026
{% endhighlight %}

##Exercise 3
This exercise will let you practice at using some of the reshaping commands, in particular the summary commands. 

1. We have an interest in man-made vs natural lakes and how they might have differing water quality.  At this point we have a single data frame `nla_data` that should make answering that question fairly straightforward. Set up a final section in your script to look at this.
2. Add some lines to your script to calculate the mean by `LAKE_ORIGIN`, for `TURB`, `NTL`, `PTL`,`CHLA`, and `SECMEAN` (hint: columns 4 through 8). Save to a data frame named `origin_mean_wq`.
3. Repeat the same analysis but for the `WSA_ECO9` ecoregions.  Save this to a data frame named `ecoregion_mean_wq`.
4. It might be interesting to compare the grouped means to the means of each value for the entire dataset.  Using `apply()`, calculate the mean wq for all lakes.  Save this as `nla_mean_wq`.






